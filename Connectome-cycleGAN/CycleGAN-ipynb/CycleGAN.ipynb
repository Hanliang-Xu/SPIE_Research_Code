{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/bkkaggle/pytorch-CycleGAN-and-pix2pix/blob/master/CycleGAN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5VIGyIus8Vr7"
   },
   "source": [
    "Take a look at the [repository](https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix) for more information"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7wNjDKdQy35h"
   },
   "source": [
    "# Install"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TRm-USlsHgEV"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'pytorch-CycleGAN-and-pix2pix'...\n",
      "remote: Enumerating objects: 2513, done.\u001b[K\n",
      "remote: Total 2513 (delta 0), reused 0 (delta 0), pack-reused 2513\u001b[K\n",
      "Receiving objects: 100% (2513/2513), 8.20 MiB | 1.51 MiB/s, done.\n",
      "Resolving deltas: 100% (1575/1575), done.\n",
      "Updating files: 100% (74/74), done.\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Pt3igws3eiVp"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir('pytorch-CycleGAN-and-pix2pix/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "z1EySlOXwwoa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch>=1.4.0 in /home-local/software/anaconda3/envs/csvToNpy/lib/python3.11/site-packages (from -r requirements.txt (line 1)) (2.0.1)\n",
      "Requirement already satisfied: torchvision>=0.5.0 in /home-local/software/anaconda3/envs/csvToNpy/lib/python3.11/site-packages (from -r requirements.txt (line 2)) (0.15.2)\n",
      "Requirement already satisfied: dominate>=2.4.0 in /home-local/software/anaconda3/envs/csvToNpy/lib/python3.11/site-packages (from -r requirements.txt (line 3)) (2.8.0)\n",
      "Requirement already satisfied: visdom>=0.1.8.8 in /home-local/software/anaconda3/envs/csvToNpy/lib/python3.11/site-packages (from -r requirements.txt (line 4)) (0.2.4)\n",
      "Requirement already satisfied: wandb in /home-local/software/anaconda3/envs/csvToNpy/lib/python3.11/site-packages (from -r requirements.txt (line 5)) (0.15.5)\n",
      "Requirement already satisfied: filelock in /home-local/software/anaconda3/envs/csvToNpy/lib/python3.11/site-packages (from torch>=1.4.0->-r requirements.txt (line 1)) (3.12.2)\n",
      "Requirement already satisfied: typing-extensions in /home-local/software/anaconda3/envs/csvToNpy/lib/python3.11/site-packages (from torch>=1.4.0->-r requirements.txt (line 1)) (4.7.1)\n",
      "Requirement already satisfied: sympy in /home-local/software/anaconda3/envs/csvToNpy/lib/python3.11/site-packages (from torch>=1.4.0->-r requirements.txt (line 1)) (1.11.1)\n",
      "Requirement already satisfied: networkx in /home-local/software/anaconda3/envs/csvToNpy/lib/python3.11/site-packages (from torch>=1.4.0->-r requirements.txt (line 1)) (2.8.4)\n",
      "Requirement already satisfied: jinja2 in /home-local/software/anaconda3/envs/csvToNpy/lib/python3.11/site-packages (from torch>=1.4.0->-r requirements.txt (line 1)) (3.1.2)\n",
      "Requirement already satisfied: numpy in /home-local/software/anaconda3/envs/csvToNpy/lib/python3.11/site-packages (from torchvision>=0.5.0->-r requirements.txt (line 2)) (1.25.0)\n",
      "Requirement already satisfied: requests in /home-local/software/anaconda3/envs/csvToNpy/lib/python3.11/site-packages (from torchvision>=0.5.0->-r requirements.txt (line 2)) (2.29.0)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /home-local/software/anaconda3/envs/csvToNpy/lib/python3.11/site-packages (from torchvision>=0.5.0->-r requirements.txt (line 2)) (9.4.0)\n",
      "Requirement already satisfied: scipy in /home-local/software/anaconda3/envs/csvToNpy/lib/python3.11/site-packages (from visdom>=0.1.8.8->-r requirements.txt (line 4)) (1.11.1)\n",
      "Requirement already satisfied: tornado in /home-local/software/anaconda3/envs/csvToNpy/lib/python3.11/site-packages (from visdom>=0.1.8.8->-r requirements.txt (line 4)) (6.2)\n",
      "Requirement already satisfied: six in /home-local/software/anaconda3/envs/csvToNpy/lib/python3.11/site-packages (from visdom>=0.1.8.8->-r requirements.txt (line 4)) (1.16.0)\n",
      "Requirement already satisfied: jsonpatch in /home-local/software/anaconda3/envs/csvToNpy/lib/python3.11/site-packages (from visdom>=0.1.8.8->-r requirements.txt (line 4)) (1.33)\n",
      "Requirement already satisfied: websocket-client in /home-local/software/anaconda3/envs/csvToNpy/lib/python3.11/site-packages (from visdom>=0.1.8.8->-r requirements.txt (line 4)) (1.6.1)\n",
      "Requirement already satisfied: Click!=8.0.0,>=7.1 in /home-local/software/anaconda3/envs/csvToNpy/lib/python3.11/site-packages (from wandb->-r requirements.txt (line 5)) (8.1.6)\n",
      "Requirement already satisfied: GitPython!=3.1.29,>=1.0.0 in /home-local/software/anaconda3/envs/csvToNpy/lib/python3.11/site-packages (from wandb->-r requirements.txt (line 5)) (3.1.32)\n",
      "Requirement already satisfied: psutil>=5.0.0 in /home-local/software/anaconda3/envs/csvToNpy/lib/python3.11/site-packages (from wandb->-r requirements.txt (line 5)) (5.9.0)\n",
      "Requirement already satisfied: sentry-sdk>=1.0.0 in /home-local/software/anaconda3/envs/csvToNpy/lib/python3.11/site-packages (from wandb->-r requirements.txt (line 5)) (1.28.1)\n",
      "Requirement already satisfied: docker-pycreds>=0.4.0 in /home-local/software/anaconda3/envs/csvToNpy/lib/python3.11/site-packages (from wandb->-r requirements.txt (line 5)) (0.4.0)\n",
      "Requirement already satisfied: PyYAML in /home-local/software/anaconda3/envs/csvToNpy/lib/python3.11/site-packages (from wandb->-r requirements.txt (line 5)) (6.0.1)\n",
      "Requirement already satisfied: pathtools in /home-local/software/anaconda3/envs/csvToNpy/lib/python3.11/site-packages (from wandb->-r requirements.txt (line 5)) (0.1.2)\n",
      "Requirement already satisfied: setproctitle in /home-local/software/anaconda3/envs/csvToNpy/lib/python3.11/site-packages (from wandb->-r requirements.txt (line 5)) (1.3.2)\n",
      "Requirement already satisfied: setuptools in /home-local/software/anaconda3/envs/csvToNpy/lib/python3.11/site-packages (from wandb->-r requirements.txt (line 5)) (67.8.0)\n",
      "Requirement already satisfied: appdirs>=1.4.3 in /home-local/software/anaconda3/envs/csvToNpy/lib/python3.11/site-packages (from wandb->-r requirements.txt (line 5)) (1.4.4)\n",
      "Requirement already satisfied: protobuf!=4.21.0,<5,>=3.19.0 in /home-local/software/anaconda3/envs/csvToNpy/lib/python3.11/site-packages (from wandb->-r requirements.txt (line 5)) (4.23.4)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in /home-local/software/anaconda3/envs/csvToNpy/lib/python3.11/site-packages (from GitPython!=3.1.29,>=1.0.0->wandb->-r requirements.txt (line 5)) (4.0.10)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home-local/software/anaconda3/envs/csvToNpy/lib/python3.11/site-packages (from requests->torchvision>=0.5.0->-r requirements.txt (line 2)) (3.1.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home-local/software/anaconda3/envs/csvToNpy/lib/python3.11/site-packages (from requests->torchvision>=0.5.0->-r requirements.txt (line 2)) (3.4)\n",
      "Collecting urllib3<1.27,>=1.21.1 (from requests->torchvision>=0.5.0->-r requirements.txt (line 2))\n",
      "  Using cached urllib3-1.26.16-py2.py3-none-any.whl (143 kB)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home-local/software/anaconda3/envs/csvToNpy/lib/python3.11/site-packages (from requests->torchvision>=0.5.0->-r requirements.txt (line 2)) (2023.5.7)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home-local/software/anaconda3/envs/csvToNpy/lib/python3.11/site-packages (from jinja2->torch>=1.4.0->-r requirements.txt (line 1)) (2.1.1)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /home-local/software/anaconda3/envs/csvToNpy/lib/python3.11/site-packages (from jsonpatch->visdom>=0.1.8.8->-r requirements.txt (line 4)) (2.4)\n",
      "Requirement already satisfied: mpmath>=0.19 in /home-local/software/anaconda3/envs/csvToNpy/lib/python3.11/site-packages (from sympy->torch>=1.4.0->-r requirements.txt (line 1)) (1.3.0)\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in /home-local/software/anaconda3/envs/csvToNpy/lib/python3.11/site-packages (from gitdb<5,>=4.0.1->GitPython!=3.1.29,>=1.0.0->wandb->-r requirements.txt (line 5)) (5.0.0)\n",
      "Installing collected packages: urllib3\n",
      "  Attempting uninstall: urllib3\n",
      "    Found existing installation: urllib3 2.0.4\n",
      "    Uninstalling urllib3-2.0.4:\n",
      "      Successfully uninstalled urllib3-2.0.4\n",
      "Successfully installed urllib3-1.26.16\n"
     ]
    }
   ],
   "source": [
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8daqlgVhw29P"
   },
   "source": [
    "# Datasets\n",
    "\n",
    "Download one of the official datasets with:\n",
    "\n",
    "-   `bash ./datasets/download_cyclegan_dataset.sh [apple2orange, summer2winter_yosemite, horse2zebra, monet2photo, cezanne2photo, ukiyoe2photo, vangogh2photo, maps, cityscapes, facades, iphone2dslr_flower, ae_photos]`\n",
    "\n",
    "Or use your own dataset by creating the appropriate folders and adding in the images.\n",
    "\n",
    "-   Create a dataset folder under `/dataset` for your dataset.\n",
    "-   Create subfolders `testA`, `testB`, `trainA`, and `trainB` under your dataset's folder. Place any images you want to transform from a to b (cat2dog) in the `testA` folder, images you want to transform from b to a (dog2cat) in the `testB` folder, and do the same for the `trainA` and `trainB` folders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vrdOettJxaCc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Specified [horse2zebra]\n",
      "WARNING: timestamping does nothing in combination with -O. See the manual\n",
      "for details.\n",
      "\n",
      "--2023-07-19 18:27:51--  http://efrosgans.eecs.berkeley.edu/cyclegan/datasets/horse2zebra.zip\n",
      "Resolving efrosgans.eecs.berkeley.edu (efrosgans.eecs.berkeley.edu)... 128.32.244.190\n",
      "Connecting to efrosgans.eecs.berkeley.edu (efrosgans.eecs.berkeley.edu)|128.32.244.190|:80... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 116867962 (111M) [application/zip]\n",
      "Saving to: ‘./datasets/horse2zebra.zip’\n",
      "\n",
      "ebra.zip             68%[============>       ]  76.17M   690KB/s    eta 50s    ^C\n"
     ]
    }
   ],
   "source": [
    "!bash ./datasets/download_cyclegan_dataset.sh horse2zebra"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gdUz4116xhpm"
   },
   "source": [
    "# Pretrained models\n",
    "\n",
    "Download one of the official pretrained models with:\n",
    "\n",
    "-   `bash ./scripts/download_cyclegan_model.sh [apple2orange, orange2apple, summer2winter_yosemite, winter2summer_yosemite, horse2zebra, zebra2horse, monet2photo, style_monet, style_cezanne, style_ukiyoe, style_vangogh, sat2map, map2sat, cityscapes_photo2label, cityscapes_label2photo, facades_photo2label, facades_label2photo, iphone2dslr_flower]`\n",
    "\n",
    "Or add your own pretrained model to `./checkpoints/{NAME}_pretrained/latest_net_G.pt`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "B75UqtKhxznS"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: available models are apple2orange, orange2apple, summer2winter_yosemite, winter2summer_yosemite, horse2zebra, zebra2horse, monet2photo, style_monet, style_cezanne, style_ukiyoe, style_vangogh, sat2map, map2sat, cityscapes_photo2label, cityscapes_label2photo, facades_photo2label, facades_label2photo, iphone2dslr_flower\n",
      "Specified [horse2zebra]\n",
      "WARNING: timestamping does nothing in combination with -O. See the manual\n",
      "for details.\n",
      "\n",
      "--2023-07-19 18:29:43--  http://efrosgans.eecs.berkeley.edu/cyclegan/pretrained_models/horse2zebra.pth\n",
      "Resolving efrosgans.eecs.berkeley.edu (efrosgans.eecs.berkeley.edu)... 128.32.244.190\n",
      "Connecting to efrosgans.eecs.berkeley.edu (efrosgans.eecs.berkeley.edu)|128.32.244.190|:80... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 45575747 (43M)\n",
      "Saving to: ‘./checkpoints/horse2zebra_pretrained/latest_net_G.pth’\n",
      "\n",
      "        ./checkpoin   1%[                    ] 621.47K  1.11MB/s               ^C\n"
     ]
    }
   ],
   "source": [
    "!bash ./scripts/download_cyclegan_model.sh horse2zebra"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yFw1kDQBx3LN"
   },
   "source": [
    "# Training\n",
    "\n",
    "-   `python train.py --dataroot ./datasets/horse2zebra --name horse2zebra --model cycle_gan`\n",
    "\n",
    "Change the `--dataroot` and `--name` to your own dataset's path and model's name. Use `--gpu_ids 0,1,..` to train on multiple GPUs and `--batch_size` to change the batch size. I've found that a batch size of 16 fits onto 4 V100s and can finish training an epoch in ~90s.\n",
    "\n",
    "Once your model has trained, copy over the last checkpoint to a format that the testing model can automatically detect:\n",
    "\n",
    "Use `cp ./checkpoints/horse2zebra/latest_net_G_A.pth ./checkpoints/horse2zebra/latest_net_G.pth` if you want to transform images from class A to class B and `cp ./checkpoints/horse2zebra/latest_net_G_B.pth ./checkpoints/horse2zebra/latest_net_G.pth` if you want to transform images from class B to class A.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0sp7TCT2x9dB"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------- Options ---------------\n",
      "               batch_size: 1                             \n",
      "                    beta1: 0.5                           \n",
      "          checkpoints_dir: ./checkpoints                 \n",
      "           continue_train: False                         \n",
      "                crop_size: 256                           \n",
      "                 dataroot: /nfs2/xuh11/Connectome/CycleGAN-ipynb/pytorch-CycleGAN-and-pix2pix/datasets/Mean_Length\t[default: None]\n",
      "             dataset_mode: unaligned                     \n",
      "                direction: AtoB                          \n",
      "              display_env: main                          \n",
      "             display_freq: 400                           \n",
      "               display_id: -1                            \t[default: 1]\n",
      "            display_ncols: 4                             \n",
      "             display_port: 8097                          \n",
      "           display_server: http://localhost              \n",
      "          display_winsize: 256                           \n",
      "                    epoch: latest                        \n",
      "              epoch_count: 1                             \n",
      "                 gan_mode: lsgan                         \n",
      "                  gpu_ids: 0                             \n",
      "                init_gain: 0.02                          \n",
      "                init_type: normal                        \n",
      "                 input_nc: 1                             \t[default: 3]\n",
      "                  isTrain: True                          \t[default: None]\n",
      "                 lambda_A: 10.0                          \n",
      "                 lambda_B: 10.0                          \n",
      "          lambda_identity: 0.5                           \n",
      "                load_iter: 0                             \t[default: 0]\n",
      "                load_size: 286                           \n",
      "                       lr: 0.0002                        \n",
      "           lr_decay_iters: 50                            \n",
      "                lr_policy: linear                        \n",
      "         max_dataset_size: inf                           \n",
      "                    model: cycle_gan                     \n",
      "                 n_epochs: 100                           \n",
      "           n_epochs_decay: 100                           \n",
      "               n_layers_D: 3                             \n",
      "                     name: BIOCARDtoVMAP_MeanLength      \t[default: experiment_name]\n",
      "                      ndf: 64                            \n",
      "                     netD: basic                         \n",
      "                     netG: resnet_9blocks                \n",
      "                      ngf: 64                            \n",
      "               no_dropout: True                          \n",
      "                  no_flip: False                         \n",
      "                  no_html: False                         \n",
      "                     norm: instance                      \n",
      "              num_threads: 4                             \n",
      "                output_nc: 1                             \t[default: 3]\n",
      "                    phase: train                         \n",
      "                pool_size: 50                            \n",
      "               preprocess: resize_and_crop               \n",
      "               print_freq: 100                           \n",
      "             save_by_iter: False                         \n",
      "          save_epoch_freq: 5                             \n",
      "         save_latest_freq: 5000                          \n",
      "           serial_batches: False                         \n",
      "                   suffix:                               \n",
      "         update_html_freq: 1000                          \n",
      "                use_wandb: False                         \n",
      "                  verbose: False                         \n",
      "       wandb_project_name: CycleGAN-and-pix2pix          \n",
      "----------------- End -------------------\n",
      "dataset [UnalignedDataset] was created\n",
      "The number of training images = 68\n",
      "initialize network with normal\n",
      "initialize network with normal\n",
      "initialize network with normal\n",
      "initialize network with normal\n",
      "model [CycleGANModel] was created\n",
      "---------- Networks initialized -------------\n",
      "[Network G_A] Total number of parameters : 11.366 M\n",
      "[Network G_B] Total number of parameters : 11.366 M\n",
      "[Network D_A] Total number of parameters : 2.763 M\n",
      "[Network D_B] Total number of parameters : 2.763 M\n",
      "-----------------------------------------------\n",
      "create web directory ./checkpoints/BIOCARDtoVMAP_MeanLength/web...\n",
      "/home-local/software/anaconda3/envs/csvToNpy/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:139: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n",
      "learning rate 0.0002000 -> 0.0002000\n",
      "End of epoch 1 / 200 \t Time Taken: 6 sec\n",
      "learning rate 0.0002000 -> 0.0002000\n",
      "(epoch: 2, iters: 32, time: 0.077, data: 0.128) D_A: 0.349 G_A: 0.518 cycle_A: 2.662 idt_A: 0.989 D_B: 0.701 G_B: 0.698 cycle_B: 1.945 idt_B: 1.338 \n",
      "End of epoch 2 / 200 \t Time Taken: 5 sec\n",
      "learning rate 0.0002000 -> 0.0002000\n",
      "(epoch: 3, iters: 64, time: 0.074, data: 0.002) D_A: 0.235 G_A: 0.283 cycle_A: 1.898 idt_A: 0.801 D_B: 0.146 G_B: 0.166 cycle_B: 1.474 idt_B: 1.051 \n",
      "End of epoch 3 / 200 \t Time Taken: 5 sec\n",
      "learning rate 0.0002000 -> 0.0002000\n",
      "End of epoch 4 / 200 \t Time Taken: 5 sec\n",
      "learning rate 0.0002000 -> 0.0002000\n",
      "(epoch: 5, iters: 28, time: 0.080, data: 0.002) D_A: 0.357 G_A: 0.046 cycle_A: 1.440 idt_A: 0.682 D_B: 0.210 G_B: 0.321 cycle_B: 1.237 idt_B: 0.808 \n",
      "saving the model at the end of epoch 5, iters 340\n",
      "End of epoch 5 / 200 \t Time Taken: 20 sec\n",
      "learning rate 0.0002000 -> 0.0002000\n",
      "(epoch: 6, iters: 60, time: 0.787, data: 0.002) D_A: 0.210 G_A: 0.337 cycle_A: 1.850 idt_A: 0.593 D_B: 0.255 G_B: 0.357 cycle_B: 1.098 idt_B: 0.936 \n",
      "End of epoch 6 / 200 \t Time Taken: 6 sec\n",
      "learning rate 0.0002000 -> 0.0002000\n",
      "End of epoch 7 / 200 \t Time Taken: 5 sec\n",
      "learning rate 0.0002000 -> 0.0002000\n",
      "(epoch: 8, iters: 24, time: 0.090, data: 0.002) D_A: 0.387 G_A: 0.085 cycle_A: 1.363 idt_A: 0.604 D_B: 0.381 G_B: 0.618 cycle_B: 1.182 idt_B: 0.645 \n",
      "End of epoch 8 / 200 \t Time Taken: 5 sec\n",
      "learning rate 0.0002000 -> 0.0002000\n",
      "(epoch: 9, iters: 56, time: 0.072, data: 0.001) D_A: 0.308 G_A: 0.635 cycle_A: 1.553 idt_A: 0.523 D_B: 0.221 G_B: 0.123 cycle_B: 1.089 idt_B: 0.758 \n",
      "End of epoch 9 / 200 \t Time Taken: 5 sec\n",
      "learning rate 0.0002000 -> 0.0002000\n",
      "saving the model at the end of epoch 10, iters 680\n",
      "End of epoch 10 / 200 \t Time Taken: 13 sec\n",
      "learning rate 0.0002000 -> 0.0002000\n",
      "(epoch: 11, iters: 20, time: 0.066, data: 0.002) D_A: 0.174 G_A: 0.241 cycle_A: 1.444 idt_A: 0.615 D_B: 0.207 G_B: 0.379 cycle_B: 1.323 idt_B: 0.659 \n",
      "End of epoch 11 / 200 \t Time Taken: 5 sec\n",
      "learning rate 0.0002000 -> 0.0002000\n",
      "(epoch: 12, iters: 52, time: 0.729, data: 0.002) D_A: 0.216 G_A: 0.469 cycle_A: 1.439 idt_A: 0.553 D_B: 0.256 G_B: 0.509 cycle_B: 1.152 idt_B: 0.691 \n",
      "End of epoch 12 / 200 \t Time Taken: 6 sec\n",
      "learning rate 0.0002000 -> 0.0002000\n",
      "End of epoch 13 / 200 \t Time Taken: 5 sec\n",
      "learning rate 0.0002000 -> 0.0002000\n",
      "(epoch: 14, iters: 16, time: 0.075, data: 0.002) D_A: 0.205 G_A: 0.667 cycle_A: 1.331 idt_A: 0.526 D_B: 0.203 G_B: 0.360 cycle_B: 1.193 idt_B: 0.624 \n",
      "End of epoch 14 / 200 \t Time Taken: 5 sec\n",
      "learning rate 0.0002000 -> 0.0002000\n",
      "(epoch: 15, iters: 48, time: 0.098, data: 0.002) D_A: 0.104 G_A: 0.139 cycle_A: 1.411 idt_A: 0.547 D_B: 0.192 G_B: 0.378 cycle_B: 1.266 idt_B: 0.611 \n",
      "saving the model at the end of epoch 15, iters 1020\n",
      "End of epoch 15 / 200 \t Time Taken: 14 sec\n",
      "learning rate 0.0002000 -> 0.0002000\n",
      "End of epoch 16 / 200 \t Time Taken: 5 sec\n",
      "learning rate 0.0002000 -> 0.0002000\n",
      "(epoch: 17, iters: 12, time: 0.090, data: 0.002) D_A: 0.216 G_A: 0.364 cycle_A: 1.228 idt_A: 0.427 D_B: 0.192 G_B: 0.376 cycle_B: 0.958 idt_B: 0.563 \n",
      "End of epoch 17 / 200 \t Time Taken: 5 sec\n",
      "learning rate 0.0002000 -> 0.0002000\n",
      "(epoch: 18, iters: 44, time: 0.760, data: 0.001) D_A: 0.120 G_A: 0.379 cycle_A: 1.314 idt_A: 0.439 D_B: 0.209 G_B: 0.620 cycle_B: 0.993 idt_B: 0.585 \n",
      "End of epoch 18 / 200 \t Time Taken: 6 sec\n",
      "learning rate 0.0002000 -> 0.0002000\n",
      "End of epoch 19 / 200 \t Time Taken: 5 sec\n",
      "learning rate 0.0002000 -> 0.0002000\n",
      "(epoch: 20, iters: 8, time: 0.066, data: 0.002) D_A: 0.068 G_A: 0.224 cycle_A: 1.306 idt_A: 0.450 D_B: 0.304 G_B: 0.547 cycle_B: 1.045 idt_B: 0.591 \n",
      "saving the model at the end of epoch 20, iters 1360\n",
      "End of epoch 20 / 200 \t Time Taken: 12 sec\n",
      "learning rate 0.0002000 -> 0.0002000\n",
      "(epoch: 21, iters: 40, time: 0.068, data: 0.000) D_A: 0.155 G_A: 0.508 cycle_A: 1.403 idt_A: 0.546 D_B: 0.232 G_B: 0.221 cycle_B: 1.171 idt_B: 0.647 \n",
      "End of epoch 21 / 200 \t Time Taken: 5 sec\n",
      "learning rate 0.0002000 -> 0.0002000\n",
      "End of epoch 22 / 200 \t Time Taken: 5 sec\n",
      "learning rate 0.0002000 -> 0.0002000\n",
      "(epoch: 23, iters: 4, time: 0.092, data: 0.002) D_A: 0.224 G_A: 0.702 cycle_A: 1.481 idt_A: 0.445 D_B: 0.387 G_B: 0.096 cycle_B: 1.066 idt_B: 0.668 \n",
      "End of epoch 23 / 200 \t Time Taken: 5 sec\n",
      "learning rate 0.0002000 -> 0.0002000\n",
      "(epoch: 24, iters: 36, time: 0.731, data: 0.000) D_A: 0.123 G_A: 0.276 cycle_A: 1.212 idt_A: 0.454 D_B: 0.353 G_B: 0.074 cycle_B: 1.026 idt_B: 0.539 \n",
      "End of epoch 24 / 200 \t Time Taken: 6 sec\n",
      "learning rate 0.0002000 -> 0.0002000\n",
      "(epoch: 25, iters: 68, time: 0.088, data: 0.002) D_A: 0.144 G_A: 0.362 cycle_A: 1.304 idt_A: 0.517 D_B: 0.236 G_B: 0.442 cycle_B: 1.245 idt_B: 0.581 \n",
      "saving the model at the end of epoch 25, iters 1700\n",
      "End of epoch 25 / 200 \t Time Taken: 20 sec\n",
      "learning rate 0.0002000 -> 0.0002000\n",
      "End of epoch 26 / 200 \t Time Taken: 5 sec\n",
      "learning rate 0.0002000 -> 0.0002000\n",
      "(epoch: 27, iters: 32, time: 0.091, data: 0.205) D_A: 0.432 G_A: 0.069 cycle_A: 1.311 idt_A: 0.476 D_B: 0.092 G_B: 0.261 cycle_B: 1.102 idt_B: 0.597 \n",
      "End of epoch 27 / 200 \t Time Taken: 5 sec\n",
      "learning rate 0.0002000 -> 0.0002000\n",
      "(epoch: 28, iters: 64, time: 0.097, data: 0.001) D_A: 0.290 G_A: 0.890 cycle_A: 1.448 idt_A: 0.508 D_B: 0.066 G_B: 0.293 cycle_B: 1.186 idt_B: 0.629 \n",
      "End of epoch 28 / 200 \t Time Taken: 5 sec\n",
      "learning rate 0.0002000 -> 0.0002000\n",
      "End of epoch 29 / 200 \t Time Taken: 5 sec\n",
      "learning rate 0.0002000 -> 0.0002000\n",
      "(epoch: 30, iters: 28, time: 0.742, data: 0.002) D_A: 0.304 G_A: 0.091 cycle_A: 1.224 idt_A: 0.432 D_B: 0.070 G_B: 0.096 cycle_B: 1.192 idt_B: 0.537 \n",
      "saving the model at the end of epoch 30, iters 2040\n",
      "End of epoch 30 / 200 \t Time Taken: 13 sec\n",
      "learning rate 0.0002000 -> 0.0002000\n",
      "(epoch: 31, iters: 60, time: 0.088, data: 0.002) D_A: 0.318 G_A: 0.657 cycle_A: 1.427 idt_A: 0.436 D_B: 0.112 G_B: 0.488 cycle_B: 0.997 idt_B: 0.620 \n",
      "End of epoch 31 / 200 \t Time Taken: 5 sec\n",
      "learning rate 0.0002000 -> 0.0002000\n",
      "End of epoch 32 / 200 \t Time Taken: 5 sec\n",
      "learning rate 0.0002000 -> 0.0002000\n",
      "(epoch: 33, iters: 24, time: 0.070, data: 0.002) D_A: 0.183 G_A: 0.565 cycle_A: 1.426 idt_A: 0.522 D_B: 0.211 G_B: 0.414 cycle_B: 1.168 idt_B: 0.626 \n",
      "End of epoch 33 / 200 \t Time Taken: 5 sec\n",
      "learning rate 0.0002000 -> 0.0002000\n",
      "(epoch: 34, iters: 56, time: 0.085, data: 0.002) D_A: 0.133 G_A: 0.406 cycle_A: 1.229 idt_A: 0.470 D_B: 0.055 G_B: 0.143 cycle_B: 0.995 idt_B: 0.558 \n",
      "End of epoch 34 / 200 \t Time Taken: 5 sec\n",
      "learning rate 0.0002000 -> 0.0002000\n",
      "saving the model at the end of epoch 35, iters 2380\n",
      "End of epoch 35 / 200 \t Time Taken: 12 sec\n",
      "learning rate 0.0002000 -> 0.0002000\n",
      "(epoch: 36, iters: 20, time: 0.790, data: 0.002) D_A: 0.114 G_A: 0.377 cycle_A: 1.345 idt_A: 0.441 D_B: 0.680 G_B: 0.058 cycle_B: 1.089 idt_B: 0.601 \n",
      "End of epoch 36 / 200 \t Time Taken: 6 sec\n",
      "learning rate 0.0002000 -> 0.0002000\n",
      "(epoch: 37, iters: 52, time: 0.082, data: 0.002) D_A: 0.162 G_A: 0.408 cycle_A: 1.191 idt_A: 0.421 D_B: 0.167 G_B: 0.787 cycle_B: 0.949 idt_B: 0.532 \n",
      "End of epoch 37 / 200 \t Time Taken: 5 sec\n",
      "learning rate 0.0002000 -> 0.0002000\n",
      "End of epoch 38 / 200 \t Time Taken: 5 sec\n",
      "learning rate 0.0002000 -> 0.0002000\n",
      "(epoch: 39, iters: 16, time: 0.083, data: 0.002) D_A: 0.465 G_A: 0.023 cycle_A: 1.553 idt_A: 0.413 D_B: 0.163 G_B: 1.171 cycle_B: 0.958 idt_B: 0.620 \n",
      "End of epoch 39 / 200 \t Time Taken: 5 sec\n",
      "learning rate 0.0002000 -> 0.0002000\n",
      "(epoch: 40, iters: 48, time: 0.087, data: 0.001) D_A: 0.137 G_A: 0.716 cycle_A: 1.336 idt_A: 0.439 D_B: 0.203 G_B: 0.824 cycle_B: 0.995 idt_B: 0.594 \n",
      "saving the model at the end of epoch 40, iters 2720\n",
      "End of epoch 40 / 200 \t Time Taken: 13 sec\n",
      "learning rate 0.0002000 -> 0.0002000\n",
      "End of epoch 41 / 200 \t Time Taken: 5 sec\n",
      "learning rate 0.0002000 -> 0.0002000\n",
      "(epoch: 42, iters: 12, time: 0.781, data: 0.002) D_A: 0.241 G_A: 0.147 cycle_A: 1.879 idt_A: 0.424 D_B: 0.054 G_B: 0.813 cycle_B: 0.934 idt_B: 0.783 \n",
      "End of epoch 42 / 200 \t Time Taken: 6 sec\n",
      "learning rate 0.0002000 -> 0.0002000\n",
      "(epoch: 43, iters: 44, time: 0.071, data: 0.002) D_A: 0.049 G_A: 0.471 cycle_A: 1.453 idt_A: 0.473 D_B: 0.073 G_B: 0.360 cycle_B: 1.249 idt_B: 0.611 \n",
      "End of epoch 43 / 200 \t Time Taken: 5 sec\n",
      "learning rate 0.0002000 -> 0.0002000\n",
      "End of epoch 44 / 200 \t Time Taken: 5 sec\n",
      "learning rate 0.0002000 -> 0.0002000\n",
      "(epoch: 45, iters: 8, time: 0.093, data: 0.002) D_A: 0.200 G_A: 0.209 cycle_A: 1.178 idt_A: 0.427 D_B: 0.047 G_B: 0.503 cycle_B: 0.980 idt_B: 0.519 \n",
      "saving the model at the end of epoch 45, iters 3060\n",
      "End of epoch 45 / 200 \t Time Taken: 12 sec\n",
      "learning rate 0.0002000 -> 0.0002000\n",
      "(epoch: 46, iters: 40, time: 0.086, data: 0.000) D_A: 0.080 G_A: 0.459 cycle_A: 1.305 idt_A: 0.602 D_B: 0.081 G_B: 0.402 cycle_B: 1.427 idt_B: 0.558 \n",
      "End of epoch 46 / 200 \t Time Taken: 5 sec\n",
      "learning rate 0.0002000 -> 0.0002000\n",
      "End of epoch 47 / 200 \t Time Taken: 5 sec\n",
      "learning rate 0.0002000 -> 0.0002000\n",
      "(epoch: 48, iters: 4, time: 0.766, data: 0.002) D_A: 0.544 G_A: 0.951 cycle_A: 1.403 idt_A: 0.452 D_B: 0.148 G_B: 0.306 cycle_B: 1.054 idt_B: 0.603 \n",
      "End of epoch 48 / 200 \t Time Taken: 6 sec\n",
      "learning rate 0.0002000 -> 0.0002000\n",
      "(epoch: 49, iters: 36, time: 0.078, data: 0.000) D_A: 0.185 G_A: 0.248 cycle_A: 1.386 idt_A: 0.467 D_B: 0.034 G_B: 0.375 cycle_B: 1.079 idt_B: 0.660 \n",
      "End of epoch 49 / 200 \t Time Taken: 5 sec\n",
      "learning rate 0.0002000 -> 0.0002000\n",
      "(epoch: 50, iters: 68, time: 0.063, data: 0.002) D_A: 0.103 G_A: 0.650 cycle_A: 1.353 idt_A: 0.403 D_B: 0.103 G_B: 0.931 cycle_B: 0.957 idt_B: 0.619 \n",
      "saving the model at the end of epoch 50, iters 3400\n",
      "End of epoch 50 / 200 \t Time Taken: 20 sec\n",
      "learning rate 0.0002000 -> 0.0002000\n",
      "End of epoch 51 / 200 \t Time Taken: 5 sec\n",
      "learning rate 0.0002000 -> 0.0002000\n",
      "(epoch: 52, iters: 32, time: 0.075, data: 0.193) D_A: 0.049 G_A: 0.594 cycle_A: 1.299 idt_A: 0.375 D_B: 0.150 G_B: 0.803 cycle_B: 0.985 idt_B: 0.564 \n",
      "End of epoch 52 / 200 \t Time Taken: 5 sec\n",
      "learning rate 0.0002000 -> 0.0002000\n",
      "(epoch: 53, iters: 64, time: 0.781, data: 0.001) D_A: 0.050 G_A: 0.658 cycle_A: 1.491 idt_A: 0.441 D_B: 0.034 G_B: 0.551 cycle_B: 1.102 idt_B: 0.651 \n",
      "End of epoch 53 / 200 \t Time Taken: 6 sec\n",
      "learning rate 0.0002000 -> 0.0002000\n",
      "End of epoch 54 / 200 \t Time Taken: 5 sec\n",
      "learning rate 0.0002000 -> 0.0002000\n",
      "(epoch: 55, iters: 28, time: 0.074, data: 0.002) D_A: 0.084 G_A: 0.465 cycle_A: 1.168 idt_A: 0.428 D_B: 0.258 G_B: 0.095 cycle_B: 0.985 idt_B: 0.546 \n",
      "saving the model at the end of epoch 55, iters 3740\n",
      "End of epoch 55 / 200 \t Time Taken: 13 sec\n",
      "learning rate 0.0002000 -> 0.0002000\n",
      "(epoch: 56, iters: 60, time: 0.083, data: 0.002) D_A: 0.159 G_A: 0.595 cycle_A: 1.687 idt_A: 0.418 D_B: 0.056 G_B: 0.163 cycle_B: 0.973 idt_B: 0.610 \n",
      "End of epoch 56 / 200 \t Time Taken: 5 sec\n",
      "learning rate 0.0002000 -> 0.0002000\n",
      "End of epoch 57 / 200 \t Time Taken: 5 sec\n",
      "learning rate 0.0002000 -> 0.0002000\n",
      "(epoch: 58, iters: 24, time: 0.087, data: 0.002) D_A: 0.069 G_A: 0.572 cycle_A: 1.239 idt_A: 0.364 D_B: 0.028 G_B: 0.132 cycle_B: 0.862 idt_B: 0.547 \n",
      "End of epoch 58 / 200 \t Time Taken: 5 sec\n",
      "learning rate 0.0002000 -> 0.0002000\n",
      "(epoch: 59, iters: 56, time: 0.900, data: 0.002) D_A: 0.053 G_A: 0.702 cycle_A: 1.241 idt_A: 0.433 D_B: 0.065 G_B: 0.567 cycle_B: 1.129 idt_B: 0.531 \n",
      "End of epoch 59 / 200 \t Time Taken: 6 sec\n",
      "learning rate 0.0002000 -> 0.0002000\n",
      "saving the model at the end of epoch 60, iters 4080\n",
      "End of epoch 60 / 200 \t Time Taken: 19 sec\n",
      "learning rate 0.0002000 -> 0.0002000\n",
      "(epoch: 61, iters: 20, time: 0.082, data: 0.002) D_A: 0.062 G_A: 0.763 cycle_A: 1.154 idt_A: 0.420 D_B: 0.031 G_B: 0.620 cycle_B: 1.068 idt_B: 0.496 \n",
      "End of epoch 61 / 200 \t Time Taken: 5 sec\n",
      "learning rate 0.0002000 -> 0.0002000\n",
      "(epoch: 62, iters: 52, time: 0.086, data: 0.002) D_A: 0.121 G_A: 0.351 cycle_A: 1.371 idt_A: 0.386 D_B: 0.046 G_B: 0.877 cycle_B: 0.942 idt_B: 0.566 \n",
      "End of epoch 62 / 200 \t Time Taken: 5 sec\n",
      "learning rate 0.0002000 -> 0.0002000\n",
      "End of epoch 63 / 200 \t Time Taken: 5 sec\n",
      "learning rate 0.0002000 -> 0.0002000\n",
      "(epoch: 64, iters: 16, time: 0.079, data: 0.001) D_A: 0.148 G_A: 0.511 cycle_A: 1.152 idt_A: 0.387 D_B: 0.072 G_B: 0.456 cycle_B: 0.985 idt_B: 0.499 \n",
      "End of epoch 64 / 200 \t Time Taken: 5 sec\n",
      "learning rate 0.0002000 -> 0.0002000\n",
      "(epoch: 65, iters: 48, time: 1.066, data: 0.001) D_A: 0.217 G_A: 0.294 cycle_A: 1.208 idt_A: 0.358 D_B: 0.046 G_B: 0.703 cycle_B: 0.862 idt_B: 0.523 \n",
      "saving the model at the end of epoch 65, iters 4420\n",
      "End of epoch 65 / 200 \t Time Taken: 14 sec\n",
      "learning rate 0.0002000 -> 0.0002000\n",
      "End of epoch 66 / 200 \t Time Taken: 5 sec\n",
      "learning rate 0.0002000 -> 0.0002000\n",
      "(epoch: 67, iters: 12, time: 0.069, data: 0.001) D_A: 0.055 G_A: 0.551 cycle_A: 1.273 idt_A: 0.486 D_B: 0.019 G_B: 0.561 cycle_B: 1.215 idt_B: 0.525 \n",
      "End of epoch 67 / 200 \t Time Taken: 5 sec\n",
      "learning rate 0.0002000 -> 0.0002000\n",
      "(epoch: 68, iters: 44, time: 0.087, data: 0.002) D_A: 0.105 G_A: 0.319 cycle_A: 1.224 idt_A: 0.455 D_B: 0.095 G_B: 0.568 cycle_B: 1.241 idt_B: 0.497 \n",
      "End of epoch 68 / 200 \t Time Taken: 5 sec\n",
      "learning rate 0.0002000 -> 0.0002000\n",
      "End of epoch 69 / 200 \t Time Taken: 5 sec\n",
      "learning rate 0.0002000 -> 0.0002000\n",
      "(epoch: 70, iters: 8, time: 0.084, data: 0.002) D_A: 0.205 G_A: 0.161 cycle_A: 1.125 idt_A: 0.402 D_B: 0.124 G_B: 0.607 cycle_B: 0.995 idt_B: 0.490 \n",
      "saving the model at the end of epoch 70, iters 4760\n",
      "End of epoch 70 / 200 \t Time Taken: 14 sec\n",
      "learning rate 0.0002000 -> 0.0002000\n",
      "(epoch: 71, iters: 40, time: 0.773, data: 0.000) D_A: 0.075 G_A: 0.591 cycle_A: 1.323 idt_A: 0.354 D_B: 0.075 G_B: 0.187 cycle_B: 0.837 idt_B: 0.551 \n",
      "End of epoch 71 / 200 \t Time Taken: 6 sec\n",
      "learning rate 0.0002000 -> 0.0002000\n",
      "End of epoch 72 / 200 \t Time Taken: 5 sec\n",
      "learning rate 0.0002000 -> 0.0002000\n",
      "(epoch: 73, iters: 4, time: 0.088, data: 0.001) D_A: 0.051 G_A: 0.423 cycle_A: 1.030 idt_A: 0.469 D_B: 0.075 G_B: 0.472 cycle_B: 1.198 idt_B: 0.430 \n",
      "End of epoch 73 / 200 \t Time Taken: 5 sec\n",
      "learning rate 0.0002000 -> 0.0002000\n",
      "(epoch: 74, iters: 36, time: 0.089, data: 0.000) D_A: 0.090 G_A: 0.398 cycle_A: 1.338 idt_A: 0.391 D_B: 0.052 G_B: 0.540 cycle_B: 0.990 idt_B: 0.542 \n",
      "saving the latest model (epoch 74, total_iters 5000)\n",
      "End of epoch 74 / 200 \t Time Taken: 9 sec\n",
      "learning rate 0.0002000 -> 0.0002000\n",
      "(epoch: 75, iters: 68, time: 0.076, data: 0.002) D_A: 0.025 G_A: 0.329 cycle_A: 1.281 idt_A: 0.536 D_B: 0.031 G_B: 0.600 cycle_B: 1.218 idt_B: 0.498 \n",
      "saving the model at the end of epoch 75, iters 5100\n",
      "End of epoch 75 / 200 \t Time Taken: 13 sec\n",
      "learning rate 0.0002000 -> 0.0002000\n",
      "End of epoch 76 / 200 \t Time Taken: 5 sec\n",
      "learning rate 0.0002000 -> 0.0002000\n",
      "(epoch: 77, iters: 32, time: 1.023, data: 0.193) D_A: 0.016 G_A: 0.824 cycle_A: 1.713 idt_A: 0.357 D_B: 0.028 G_B: 0.800 cycle_B: 0.882 idt_B: 0.672 \n",
      "End of epoch 77 / 200 \t Time Taken: 6 sec\n",
      "learning rate 0.0002000 -> 0.0002000\n",
      "(epoch: 78, iters: 64, time: 0.067, data: 0.002) D_A: 0.033 G_A: 0.972 cycle_A: 1.581 idt_A: 0.450 D_B: 0.081 G_B: 0.621 cycle_B: 1.151 idt_B: 0.506 \n",
      "End of epoch 78 / 200 \t Time Taken: 5 sec\n",
      "learning rate 0.0002000 -> 0.0002000\n",
      "End of epoch 79 / 200 \t Time Taken: 5 sec\n",
      "learning rate 0.0002000 -> 0.0002000\n",
      "(epoch: 80, iters: 28, time: 0.072, data: 0.002) D_A: 0.034 G_A: 0.257 cycle_A: 1.112 idt_A: 0.439 D_B: 0.084 G_B: 0.520 cycle_B: 1.135 idt_B: 0.421 \n",
      "saving the model at the end of epoch 80, iters 5440\n",
      "End of epoch 80 / 200 \t Time Taken: 12 sec\n",
      "learning rate 0.0002000 -> 0.0002000\n",
      "(epoch: 81, iters: 60, time: 0.083, data: 0.002) D_A: 0.042 G_A: 0.687 cycle_A: 0.965 idt_A: 0.383 D_B: 0.208 G_B: 0.999 cycle_B: 1.011 idt_B: 0.382 \n",
      "End of epoch 81 / 200 \t Time Taken: 5 sec\n",
      "learning rate 0.0002000 -> 0.0002000\n",
      "End of epoch 82 / 200 \t Time Taken: 5 sec\n",
      "learning rate 0.0002000 -> 0.0002000\n",
      "(epoch: 83, iters: 24, time: 0.806, data: 0.002) D_A: 0.025 G_A: 0.774 cycle_A: 1.609 idt_A: 0.365 D_B: 0.051 G_B: 0.701 cycle_B: 0.937 idt_B: 0.553 \n",
      "End of epoch 83 / 200 \t Time Taken: 6 sec\n",
      "learning rate 0.0002000 -> 0.0002000\n",
      "(epoch: 84, iters: 56, time: 0.090, data: 0.002) D_A: 0.033 G_A: 1.080 cycle_A: 1.195 idt_A: 0.453 D_B: 0.015 G_B: 0.293 cycle_B: 1.210 idt_B: 0.442 \n",
      "End of epoch 84 / 200 \t Time Taken: 5 sec\n",
      "learning rate 0.0002000 -> 0.0002000\n",
      "saving the model at the end of epoch 85, iters 5780\n",
      "End of epoch 85 / 200 \t Time Taken: 16 sec\n",
      "learning rate 0.0002000 -> 0.0002000\n",
      "(epoch: 86, iters: 20, time: 0.089, data: 0.002) D_A: 0.147 G_A: 1.000 cycle_A: 1.179 idt_A: 0.335 D_B: 0.308 G_B: 1.611 cycle_B: 0.977 idt_B: 0.433 \n",
      "End of epoch 86 / 200 \t Time Taken: 5 sec\n",
      "learning rate 0.0002000 -> 0.0002000\n",
      "(epoch: 87, iters: 52, time: 0.077, data: 0.001) D_A: 0.087 G_A: 0.258 cycle_A: 1.055 idt_A: 0.298 D_B: 0.086 G_B: 0.372 cycle_B: 0.852 idt_B: 0.391 \n",
      "End of epoch 87 / 200 \t Time Taken: 5 sec\n",
      "learning rate 0.0002000 -> 0.0002000\n",
      "End of epoch 88 / 200 \t Time Taken: 5 sec\n",
      "learning rate 0.0002000 -> 0.0002000\n",
      "(epoch: 89, iters: 16, time: 1.155, data: 0.001) D_A: 0.035 G_A: 0.265 cycle_A: 1.013 idt_A: 0.339 D_B: 0.057 G_B: 0.486 cycle_B: 0.877 idt_B: 0.367 \n",
      "End of epoch 89 / 200 \t Time Taken: 6 sec\n",
      "learning rate 0.0002000 -> 0.0002000\n",
      "(epoch: 90, iters: 48, time: 0.092, data: 0.002) D_A: 0.027 G_A: 0.386 cycle_A: 1.133 idt_A: 0.323 D_B: 0.063 G_B: 0.454 cycle_B: 0.894 idt_B: 0.379 \n",
      "saving the model at the end of epoch 90, iters 6120\n",
      "End of epoch 90 / 200 \t Time Taken: 14 sec\n",
      "learning rate 0.0002000 -> 0.0002000\n",
      "End of epoch 91 / 200 \t Time Taken: 5 sec\n",
      "learning rate 0.0002000 -> 0.0002000\n",
      "(epoch: 92, iters: 12, time: 0.086, data: 0.002) D_A: 0.052 G_A: 0.857 cycle_A: 1.335 idt_A: 0.384 D_B: 0.076 G_B: 0.626 cycle_B: 1.111 idt_B: 0.398 \n",
      "End of epoch 92 / 200 \t Time Taken: 5 sec\n",
      "learning rate 0.0002000 -> 0.0002000\n",
      "(epoch: 93, iters: 44, time: 0.096, data: 0.002) D_A: 0.040 G_A: 0.591 cycle_A: 1.301 idt_A: 0.311 D_B: 0.045 G_B: 0.548 cycle_B: 0.929 idt_B: 0.380 \n",
      "End of epoch 93 / 200 \t Time Taken: 5 sec\n",
      "learning rate 0.0002000 -> 0.0002000\n",
      "End of epoch 94 / 200 \t Time Taken: 5 sec\n",
      "learning rate 0.0002000 -> 0.0002000\n",
      "(epoch: 95, iters: 8, time: 0.825, data: 0.002) D_A: 0.019 G_A: 1.083 cycle_A: 1.085 idt_A: 0.417 D_B: 0.026 G_B: 0.563 cycle_B: 1.161 idt_B: 0.379 \n",
      "saving the model at the end of epoch 95, iters 6460\n",
      "End of epoch 95 / 200 \t Time Taken: 13 sec\n",
      "learning rate 0.0002000 -> 0.0002000\n",
      "(epoch: 96, iters: 40, time: 0.079, data: 0.000) D_A: 0.038 G_A: 0.721 cycle_A: 1.246 idt_A: 0.324 D_B: 0.015 G_B: 0.904 cycle_B: 1.104 idt_B: 0.418 \n",
      "End of epoch 96 / 200 \t Time Taken: 5 sec\n",
      "learning rate 0.0002000 -> 0.0002000\n",
      "End of epoch 97 / 200 \t Time Taken: 5 sec\n",
      "learning rate 0.0002000 -> 0.0002000\n",
      "(epoch: 98, iters: 4, time: 0.066, data: 0.003) D_A: 0.046 G_A: 0.965 cycle_A: 1.346 idt_A: 0.348 D_B: 0.022 G_B: 0.706 cycle_B: 1.180 idt_B: 0.407 \n",
      "End of epoch 98 / 200 \t Time Taken: 5 sec\n",
      "learning rate 0.0002000 -> 0.0002000\n",
      "(epoch: 99, iters: 36, time: 0.072, data: 0.000) D_A: 0.037 G_A: 0.935 cycle_A: 1.341 idt_A: 0.337 D_B: 0.016 G_B: 0.839 cycle_B: 0.990 idt_B: 0.391 \n",
      "End of epoch 99 / 200 \t Time Taken: 5 sec\n",
      "learning rate 0.0002000 -> 0.0001980\n",
      "(epoch: 100, iters: 68, time: 0.831, data: 0.002) D_A: 0.032 G_A: 0.797 cycle_A: 1.192 idt_A: 0.328 D_B: 0.020 G_B: 0.666 cycle_B: 0.937 idt_B: 0.354 \n",
      "saving the model at the end of epoch 100, iters 6800\n",
      "End of epoch 100 / 200 \t Time Taken: 13 sec\n",
      "learning rate 0.0001980 -> 0.0001960\n",
      "End of epoch 101 / 200 \t Time Taken: 5 sec\n",
      "learning rate 0.0001960 -> 0.0001941\n",
      "(epoch: 102, iters: 32, time: 0.079, data: 0.203) D_A: 0.008 G_A: 1.065 cycle_A: 1.300 idt_A: 0.532 D_B: 0.016 G_B: 0.853 cycle_B: 1.506 idt_B: 0.348 \n",
      "End of epoch 102 / 200 \t Time Taken: 5 sec\n",
      "learning rate 0.0001941 -> 0.0001921\n",
      "(epoch: 103, iters: 64, time: 0.094, data: 0.001) D_A: 0.061 G_A: 0.554 cycle_A: 1.144 idt_A: 0.316 D_B: 0.059 G_B: 0.930 cycle_B: 1.020 idt_B: 0.319 \n",
      "End of epoch 103 / 200 \t Time Taken: 5 sec\n",
      "learning rate 0.0001921 -> 0.0001901\n",
      "End of epoch 104 / 200 \t Time Taken: 5 sec\n",
      "learning rate 0.0001901 -> 0.0001881\n",
      "(epoch: 105, iters: 28, time: 0.076, data: 0.002) D_A: 0.037 G_A: 0.610 cycle_A: 1.230 idt_A: 0.264 D_B: 0.168 G_B: 0.933 cycle_B: 0.856 idt_B: 0.297 \n",
      "saving the model at the end of epoch 105, iters 7140\n",
      "End of epoch 105 / 200 \t Time Taken: 17 sec\n",
      "learning rate 0.0001881 -> 0.0001861\n",
      "(epoch: 106, iters: 60, time: 0.999, data: 0.001) D_A: 0.031 G_A: 0.318 cycle_A: 1.167 idt_A: 0.262 D_B: 0.167 G_B: 0.940 cycle_B: 0.852 idt_B: 0.309 \n",
      "End of epoch 106 / 200 \t Time Taken: 6 sec\n",
      "learning rate 0.0001861 -> 0.0001842\n",
      "End of epoch 107 / 200 \t Time Taken: 5 sec\n",
      "learning rate 0.0001842 -> 0.0001822\n",
      "(epoch: 108, iters: 24, time: 0.074, data: 0.002) D_A: 0.022 G_A: 0.880 cycle_A: 1.164 idt_A: 0.279 D_B: 0.025 G_B: 0.954 cycle_B: 1.005 idt_B: 0.297 \n",
      "End of epoch 108 / 200 \t Time Taken: 5 sec\n",
      "learning rate 0.0001822 -> 0.0001802\n",
      "(epoch: 109, iters: 56, time: 0.074, data: 0.002) D_A: 0.072 G_A: 1.018 cycle_A: 1.047 idt_A: 0.282 D_B: 0.056 G_B: 0.786 cycle_B: 0.962 idt_B: 0.297 \n",
      "End of epoch 109 / 200 \t Time Taken: 5 sec\n",
      "learning rate 0.0001802 -> 0.0001782\n",
      "saving the model at the end of epoch 110, iters 7480\n",
      "End of epoch 110 / 200 \t Time Taken: 18 sec\n",
      "learning rate 0.0001782 -> 0.0001762\n",
      "(epoch: 111, iters: 20, time: 0.096, data: 0.002) D_A: 0.066 G_A: 0.863 cycle_A: 1.180 idt_A: 0.266 D_B: 0.137 G_B: 1.265 cycle_B: 0.941 idt_B: 0.382 \n",
      "End of epoch 111 / 200 \t Time Taken: 5 sec\n",
      "learning rate 0.0001762 -> 0.0001743\n",
      "(epoch: 112, iters: 52, time: 1.014, data: 0.002) D_A: 0.045 G_A: 0.682 cycle_A: 1.087 idt_A: 0.257 D_B: 0.179 G_B: 0.235 cycle_B: 0.915 idt_B: 0.294 \n",
      "End of epoch 112 / 200 \t Time Taken: 6 sec\n",
      "learning rate 0.0001743 -> 0.0001723\n",
      "End of epoch 113 / 200 \t Time Taken: 5 sec\n",
      "learning rate 0.0001723 -> 0.0001703\n",
      "(epoch: 114, iters: 16, time: 0.078, data: 0.001) D_A: 0.077 G_A: 0.615 cycle_A: 1.129 idt_A: 0.301 D_B: 0.095 G_B: 0.767 cycle_B: 1.040 idt_B: 0.342 \n",
      "End of epoch 114 / 200 \t Time Taken: 5 sec\n",
      "learning rate 0.0001703 -> 0.0001683\n",
      "(epoch: 115, iters: 48, time: 0.073, data: 0.002) D_A: 0.023 G_A: 0.609 cycle_A: 1.351 idt_A: 0.231 D_B: 0.051 G_B: 0.894 cycle_B: 0.980 idt_B: 0.351 \n",
      "saving the model at the end of epoch 115, iters 7820\n",
      "End of epoch 115 / 200 \t Time Taken: 12 sec\n",
      "learning rate 0.0001683 -> 0.0001663\n",
      "End of epoch 116 / 200 \t Time Taken: 5 sec\n",
      "learning rate 0.0001663 -> 0.0001644\n",
      "(epoch: 117, iters: 12, time: 0.073, data: 0.002) D_A: 0.049 G_A: 0.871 cycle_A: 1.054 idt_A: 0.233 D_B: 0.044 G_B: 0.984 cycle_B: 0.870 idt_B: 0.273 \n",
      "End of epoch 117 / 200 \t Time Taken: 5 sec\n",
      "learning rate 0.0001644 -> 0.0001624\n",
      "(epoch: 118, iters: 44, time: 0.984, data: 0.001) D_A: 0.021 G_A: 0.200 cycle_A: 1.135 idt_A: 0.233 D_B: 0.081 G_B: 0.439 cycle_B: 0.851 idt_B: 0.291 \n",
      "End of epoch 118 / 200 \t Time Taken: 6 sec\n",
      "learning rate 0.0001624 -> 0.0001604\n",
      "End of epoch 119 / 200 \t Time Taken: 5 sec\n",
      "learning rate 0.0001604 -> 0.0001584\n",
      "(epoch: 120, iters: 8, time: 0.073, data: 0.002) D_A: 0.014 G_A: 0.802 cycle_A: 1.134 idt_A: 0.278 D_B: 0.056 G_B: 0.563 cycle_B: 1.163 idt_B: 0.282 \n",
      "saving the model at the end of epoch 120, iters 8160\n",
      "End of epoch 120 / 200 \t Time Taken: 20 sec\n",
      "learning rate 0.0001584 -> 0.0001564\n",
      "(epoch: 121, iters: 40, time: 0.073, data: 0.000) D_A: 0.021 G_A: 0.840 cycle_A: 1.198 idt_A: 0.312 D_B: 0.014 G_B: 0.744 cycle_B: 1.162 idt_B: 0.292 \n",
      "End of epoch 121 / 200 \t Time Taken: 5 sec\n",
      "learning rate 0.0001564 -> 0.0001545\n",
      "End of epoch 122 / 200 \t Time Taken: 5 sec\n",
      "learning rate 0.0001545 -> 0.0001525\n",
      "(epoch: 123, iters: 4, time: 0.079, data: 0.002) D_A: 0.123 G_A: 0.291 cycle_A: 1.014 idt_A: 0.221 D_B: 0.098 G_B: 0.401 cycle_B: 0.833 idt_B: 0.232 \n",
      "End of epoch 123 / 200 \t Time Taken: 5 sec\n",
      "learning rate 0.0001525 -> 0.0001505\n",
      "(epoch: 124, iters: 36, time: 1.108, data: 0.000) D_A: 0.039 G_A: 0.773 cycle_A: 1.061 idt_A: 0.239 D_B: 0.040 G_B: 0.827 cycle_B: 0.941 idt_B: 0.244 \n",
      "End of epoch 124 / 200 \t Time Taken: 6 sec\n",
      "learning rate 0.0001505 -> 0.0001485\n",
      "(epoch: 125, iters: 68, time: 0.068, data: 0.002) D_A: 0.034 G_A: 0.736 cycle_A: 1.041 idt_A: 0.221 D_B: 0.013 G_B: 0.828 cycle_B: 0.838 idt_B: 0.237 \n",
      "saving the model at the end of epoch 125, iters 8500\n",
      "End of epoch 125 / 200 \t Time Taken: 12 sec\n",
      "learning rate 0.0001485 -> 0.0001465\n",
      "End of epoch 126 / 200 \t Time Taken: 5 sec\n",
      "learning rate 0.0001465 -> 0.0001446\n",
      "(epoch: 127, iters: 32, time: 0.066, data: 0.199) D_A: 0.103 G_A: 1.045 cycle_A: 1.058 idt_A: 0.210 D_B: 0.026 G_B: 0.263 cycle_B: 0.829 idt_B: 0.243 \n",
      "End of epoch 127 / 200 \t Time Taken: 5 sec\n",
      "learning rate 0.0001446 -> 0.0001426\n",
      "(epoch: 128, iters: 64, time: 0.087, data: 0.002) D_A: 0.030 G_A: 0.775 cycle_A: 1.119 idt_A: 0.309 D_B: 0.054 G_B: 0.728 cycle_B: 0.996 idt_B: 0.251 \n",
      "End of epoch 128 / 200 \t Time Taken: 5 sec\n",
      "learning rate 0.0001426 -> 0.0001406\n",
      "End of epoch 129 / 200 \t Time Taken: 5 sec\n",
      "learning rate 0.0001406 -> 0.0001386\n",
      "(epoch: 130, iters: 28, time: 0.844, data: 0.002) D_A: 0.011 G_A: 0.812 cycle_A: 1.045 idt_A: 0.250 D_B: 0.089 G_B: 0.827 cycle_B: 0.856 idt_B: 0.274 \n",
      "saving the model at the end of epoch 130, iters 8840\n",
      "End of epoch 130 / 200 \t Time Taken: 22 sec\n",
      "learning rate 0.0001386 -> 0.0001366\n",
      "(epoch: 131, iters: 60, time: 0.074, data: 0.002) D_A: 0.013 G_A: 0.812 cycle_A: 0.887 idt_A: 0.276 D_B: 0.121 G_B: 0.869 cycle_B: 1.103 idt_B: 0.225 \n",
      "End of epoch 131 / 200 \t Time Taken: 5 sec\n",
      "learning rate 0.0001366 -> 0.0001347\n",
      "End of epoch 132 / 200 \t Time Taken: 5 sec\n",
      "learning rate 0.0001347 -> 0.0001327\n",
      "(epoch: 133, iters: 24, time: 0.067, data: 0.001) D_A: 0.069 G_A: 0.819 cycle_A: 0.970 idt_A: 0.218 D_B: 0.020 G_B: 0.793 cycle_B: 0.878 idt_B: 0.215 \n",
      "End of epoch 133 / 200 \t Time Taken: 5 sec\n",
      "learning rate 0.0001327 -> 0.0001307\n",
      "(epoch: 134, iters: 56, time: 0.084, data: 0.001) D_A: 0.092 G_A: 0.369 cycle_A: 0.835 idt_A: 0.228 D_B: 0.016 G_B: 0.691 cycle_B: 0.962 idt_B: 0.204 \n",
      "End of epoch 134 / 200 \t Time Taken: 5 sec\n",
      "learning rate 0.0001307 -> 0.0001287\n",
      "saving the model at the end of epoch 135, iters 9180\n",
      "End of epoch 135 / 200 \t Time Taken: 16 sec\n",
      "learning rate 0.0001287 -> 0.0001267\n",
      "(epoch: 136, iters: 20, time: 0.982, data: 0.002) D_A: 0.017 G_A: 0.762 cycle_A: 1.044 idt_A: 0.218 D_B: 0.011 G_B: 0.825 cycle_B: 0.855 idt_B: 0.218 \n",
      "End of epoch 136 / 200 \t Time Taken: 6 sec\n",
      "learning rate 0.0001267 -> 0.0001248\n",
      "(epoch: 137, iters: 52, time: 0.075, data: 0.002) D_A: 0.008 G_A: 0.910 cycle_A: 1.034 idt_A: 0.212 D_B: 0.021 G_B: 0.579 cycle_B: 0.940 idt_B: 0.228 \n",
      "End of epoch 137 / 200 \t Time Taken: 5 sec\n",
      "learning rate 0.0001248 -> 0.0001228\n",
      "End of epoch 138 / 200 \t Time Taken: 5 sec\n",
      "learning rate 0.0001228 -> 0.0001208\n",
      "(epoch: 139, iters: 16, time: 0.088, data: 0.002) D_A: 0.050 G_A: 0.809 cycle_A: 1.029 idt_A: 0.266 D_B: 0.072 G_B: 0.465 cycle_B: 1.077 idt_B: 0.225 \n",
      "End of epoch 139 / 200 \t Time Taken: 5 sec\n",
      "learning rate 0.0001208 -> 0.0001188\n",
      "(epoch: 140, iters: 48, time: 0.071, data: 0.002) D_A: 0.032 G_A: 0.609 cycle_A: 1.035 idt_A: 0.213 D_B: 0.068 G_B: 0.436 cycle_B: 0.880 idt_B: 0.230 \n",
      "saving the model at the end of epoch 140, iters 9520\n",
      "End of epoch 140 / 200 \t Time Taken: 12 sec\n",
      "learning rate 0.0001188 -> 0.0001168\n",
      "End of epoch 141 / 200 \t Time Taken: 5 sec\n",
      "learning rate 0.0001168 -> 0.0001149\n",
      "(epoch: 142, iters: 12, time: 1.075, data: 0.002) D_A: 0.053 G_A: 0.522 cycle_A: 1.110 idt_A: 0.206 D_B: 0.042 G_B: 0.672 cycle_B: 0.855 idt_B: 0.238 \n",
      "End of epoch 142 / 200 \t Time Taken: 6 sec\n",
      "learning rate 0.0001149 -> 0.0001129\n",
      "(epoch: 143, iters: 44, time: 0.074, data: 0.002) D_A: 0.025 G_A: 0.797 cycle_A: 0.901 idt_A: 0.214 D_B: 0.046 G_B: 0.351 cycle_B: 0.852 idt_B: 0.216 \n",
      "End of epoch 143 / 200 \t Time Taken: 5 sec\n",
      "learning rate 0.0001129 -> 0.0001109\n",
      "End of epoch 144 / 200 \t Time Taken: 5 sec\n",
      "learning rate 0.0001109 -> 0.0001089\n",
      "(epoch: 145, iters: 8, time: 0.065, data: 0.002) D_A: 0.011 G_A: 0.681 cycle_A: 1.010 idt_A: 0.194 D_B: 0.014 G_B: 0.804 cycle_B: 0.912 idt_B: 0.226 \n",
      "saving the model at the end of epoch 145, iters 9860\n",
      "End of epoch 145 / 200 \t Time Taken: 14 sec\n",
      "learning rate 0.0001089 -> 0.0001069\n",
      "(epoch: 146, iters: 40, time: 0.084, data: 0.000) D_A: 0.030 G_A: 0.943 cycle_A: 0.866 idt_A: 0.226 D_B: 0.075 G_B: 0.830 cycle_B: 0.964 idt_B: 0.188 \n",
      "End of epoch 146 / 200 \t Time Taken: 5 sec\n",
      "learning rate 0.0001069 -> 0.0001050\n",
      "End of epoch 147 / 200 \t Time Taken: 5 sec\n",
      "learning rate 0.0001050 -> 0.0001030\n",
      "(epoch: 148, iters: 4, time: 0.848, data: 0.002) D_A: 0.014 G_A: 0.639 cycle_A: 1.072 idt_A: 0.218 D_B: 0.086 G_B: 0.781 cycle_B: 1.274 idt_B: 0.196 \n",
      "saving the latest model (epoch 148, total_iters 10000)\n",
      "End of epoch 148 / 200 \t Time Taken: 10 sec\n",
      "learning rate 0.0001030 -> 0.0001010\n",
      "(epoch: 149, iters: 36, time: 0.073, data: 0.000) D_A: 0.017 G_A: 0.795 cycle_A: 0.982 idt_A: 0.212 D_B: 0.041 G_B: 0.626 cycle_B: 0.967 idt_B: 0.235 \n",
      "End of epoch 149 / 200 \t Time Taken: 5 sec\n",
      "learning rate 0.0001010 -> 0.0000990\n",
      "(epoch: 150, iters: 68, time: 0.075, data: 0.002) D_A: 0.023 G_A: 0.797 cycle_A: 1.079 idt_A: 0.187 D_B: 0.073 G_B: 0.421 cycle_B: 0.872 idt_B: 0.203 \n",
      "saving the model at the end of epoch 150, iters 10200\n",
      "End of epoch 150 / 200 \t Time Taken: 20 sec\n",
      "learning rate 0.0000990 -> 0.0000970\n",
      "End of epoch 151 / 200 \t Time Taken: 5 sec\n",
      "learning rate 0.0000970 -> 0.0000950\n",
      "(epoch: 152, iters: 32, time: 0.083, data: 0.201) D_A: 0.019 G_A: 0.578 cycle_A: 0.885 idt_A: 0.190 D_B: 0.076 G_B: 0.986 cycle_B: 0.878 idt_B: 0.183 \n",
      "End of epoch 152 / 200 \t Time Taken: 5 sec\n",
      "learning rate 0.0000950 -> 0.0000931\n",
      "(epoch: 153, iters: 64, time: 0.747, data: 0.001) D_A: 0.038 G_A: 0.967 cycle_A: 0.925 idt_A: 0.222 D_B: 0.036 G_B: 0.661 cycle_B: 1.034 idt_B: 0.225 \n",
      "End of epoch 153 / 200 \t Time Taken: 6 sec\n",
      "learning rate 0.0000931 -> 0.0000911\n",
      "End of epoch 154 / 200 \t Time Taken: 5 sec\n",
      "learning rate 0.0000911 -> 0.0000891\n",
      "(epoch: 155, iters: 28, time: 0.081, data: 0.001) D_A: 0.014 G_A: 0.740 cycle_A: 0.988 idt_A: 0.184 D_B: 0.012 G_B: 0.794 cycle_B: 0.929 idt_B: 0.235 \n",
      "saving the model at the end of epoch 155, iters 10540\n",
      "End of epoch 155 / 200 \t Time Taken: 12 sec\n",
      "learning rate 0.0000891 -> 0.0000871\n",
      "(epoch: 156, iters: 60, time: 0.064, data: 0.002) D_A: 0.017 G_A: 0.837 cycle_A: 1.009 idt_A: 0.195 D_B: 0.033 G_B: 0.600 cycle_B: 0.995 idt_B: 0.226 \n",
      "End of epoch 156 / 200 \t Time Taken: 5 sec\n",
      "learning rate 0.0000871 -> 0.0000851\n",
      "End of epoch 157 / 200 \t Time Taken: 5 sec\n",
      "learning rate 0.0000851 -> 0.0000832\n",
      "(epoch: 158, iters: 24, time: 0.088, data: 0.002) D_A: 0.010 G_A: 1.012 cycle_A: 1.079 idt_A: 0.217 D_B: 0.024 G_B: 0.750 cycle_B: 1.119 idt_B: 0.218 \n",
      "End of epoch 158 / 200 \t Time Taken: 5 sec\n",
      "learning rate 0.0000832 -> 0.0000812\n",
      "(epoch: 159, iters: 56, time: 0.685, data: 0.001) D_A: 0.010 G_A: 0.616 cycle_A: 0.829 idt_A: 0.181 D_B: 0.029 G_B: 0.613 cycle_B: 0.884 idt_B: 0.195 \n",
      "End of epoch 159 / 200 \t Time Taken: 6 sec\n",
      "learning rate 0.0000812 -> 0.0000792\n",
      "saving the model at the end of epoch 160, iters 10880\n",
      "End of epoch 160 / 200 \t Time Taken: 22 sec\n",
      "learning rate 0.0000792 -> 0.0000772\n",
      "(epoch: 161, iters: 20, time: 0.073, data: 0.001) D_A: 0.019 G_A: 0.874 cycle_A: 0.820 idt_A: 0.180 D_B: 0.013 G_B: 0.895 cycle_B: 0.857 idt_B: 0.174 \n",
      "End of epoch 161 / 200 \t Time Taken: 5 sec\n",
      "learning rate 0.0000772 -> 0.0000752\n",
      "(epoch: 162, iters: 52, time: 0.082, data: 0.002) D_A: 0.008 G_A: 0.586 cycle_A: 0.774 idt_A: 0.175 D_B: 0.008 G_B: 0.825 cycle_B: 0.797 idt_B: 0.171 \n",
      "End of epoch 162 / 200 \t Time Taken: 5 sec\n",
      "learning rate 0.0000752 -> 0.0000733\n",
      "End of epoch 163 / 200 \t Time Taken: 5 sec\n",
      "learning rate 0.0000733 -> 0.0000713\n",
      "(epoch: 164, iters: 16, time: 0.074, data: 0.002) D_A: 0.011 G_A: 0.768 cycle_A: 0.829 idt_A: 0.179 D_B: 0.021 G_B: 0.679 cycle_B: 0.806 idt_B: 0.179 \n",
      "End of epoch 164 / 200 \t Time Taken: 5 sec\n",
      "learning rate 0.0000713 -> 0.0000693\n",
      "(epoch: 165, iters: 48, time: 1.204, data: 0.001) D_A: 0.010 G_A: 0.920 cycle_A: 0.780 idt_A: 0.174 D_B: 0.007 G_B: 0.734 cycle_B: 0.879 idt_B: 0.165 \n",
      "saving the model at the end of epoch 165, iters 11220\n",
      "End of epoch 165 / 200 \t Time Taken: 23 sec\n",
      "learning rate 0.0000693 -> 0.0000673\n",
      "End of epoch 166 / 200 \t Time Taken: 5 sec\n",
      "learning rate 0.0000673 -> 0.0000653\n",
      "(epoch: 167, iters: 12, time: 0.071, data: 0.002) D_A: 0.025 G_A: 0.783 cycle_A: 0.792 idt_A: 0.178 D_B: 0.033 G_B: 0.583 cycle_B: 0.954 idt_B: 0.167 \n",
      "End of epoch 167 / 200 \t Time Taken: 5 sec\n",
      "learning rate 0.0000653 -> 0.0000634\n",
      "(epoch: 168, iters: 44, time: 0.081, data: 0.002) D_A: 0.010 G_A: 0.840 cycle_A: 0.885 idt_A: 0.167 D_B: 0.011 G_B: 0.874 cycle_B: 0.835 idt_B: 0.184 \n",
      "End of epoch 168 / 200 \t Time Taken: 5 sec\n",
      "learning rate 0.0000634 -> 0.0000614\n",
      "End of epoch 169 / 200 \t Time Taken: 5 sec\n",
      "learning rate 0.0000614 -> 0.0000594\n",
      "(epoch: 170, iters: 8, time: 0.081, data: 0.002) D_A: 0.011 G_A: 0.764 cycle_A: 0.832 idt_A: 0.161 D_B: 0.036 G_B: 0.609 cycle_B: 0.918 idt_B: 0.164 \n",
      "saving the model at the end of epoch 170, iters 11560\n",
      "End of epoch 170 / 200 \t Time Taken: 13 sec\n",
      "learning rate 0.0000594 -> 0.0000574\n",
      "(epoch: 171, iters: 40, time: 1.156, data: 0.000) D_A: 0.003 G_A: 0.962 cycle_A: 0.888 idt_A: 0.175 D_B: 0.007 G_B: 0.806 cycle_B: 0.816 idt_B: 0.171 \n",
      "End of epoch 171 / 200 \t Time Taken: 6 sec\n",
      "learning rate 0.0000574 -> 0.0000554\n",
      "End of epoch 172 / 200 \t Time Taken: 5 sec\n",
      "learning rate 0.0000554 -> 0.0000535\n",
      "(epoch: 173, iters: 4, time: 0.067, data: 0.002) D_A: 0.012 G_A: 0.758 cycle_A: 0.868 idt_A: 0.157 D_B: 0.019 G_B: 0.843 cycle_B: 0.766 idt_B: 0.154 \n",
      "End of epoch 173 / 200 \t Time Taken: 5 sec\n",
      "learning rate 0.0000535 -> 0.0000515\n",
      "(epoch: 174, iters: 36, time: 0.081, data: 0.000) D_A: 0.034 G_A: 0.877 cycle_A: 0.835 idt_A: 0.163 D_B: 0.015 G_B: 0.706 cycle_B: 0.799 idt_B: 0.162 \n",
      "End of epoch 174 / 200 \t Time Taken: 5 sec\n",
      "learning rate 0.0000515 -> 0.0000495\n",
      "(epoch: 175, iters: 68, time: 0.074, data: 0.002) D_A: 0.023 G_A: 0.877 cycle_A: 0.768 idt_A: 0.163 D_B: 0.041 G_B: 0.827 cycle_B: 0.905 idt_B: 0.154 \n",
      "saving the model at the end of epoch 175, iters 11900\n",
      "End of epoch 175 / 200 \t Time Taken: 13 sec\n",
      "learning rate 0.0000495 -> 0.0000475\n",
      "End of epoch 176 / 200 \t Time Taken: 5 sec\n",
      "learning rate 0.0000475 -> 0.0000455\n",
      "(epoch: 177, iters: 32, time: 1.147, data: 0.203) D_A: 0.006 G_A: 0.762 cycle_A: 0.873 idt_A: 0.161 D_B: 0.006 G_B: 0.843 cycle_B: 0.869 idt_B: 0.170 \n",
      "End of epoch 177 / 200 \t Time Taken: 6 sec\n",
      "learning rate 0.0000455 -> 0.0000436\n",
      "(epoch: 178, iters: 64, time: 0.076, data: 0.002) D_A: 0.006 G_A: 0.854 cycle_A: 0.732 idt_A: 0.151 D_B: 0.003 G_B: 0.829 cycle_B: 0.739 idt_B: 0.138 \n",
      "End of epoch 178 / 200 \t Time Taken: 5 sec\n",
      "learning rate 0.0000436 -> 0.0000416\n",
      "End of epoch 179 / 200 \t Time Taken: 5 sec\n",
      "learning rate 0.0000416 -> 0.0000396\n",
      "(epoch: 180, iters: 28, time: 0.085, data: 0.002) D_A: 0.005 G_A: 0.896 cycle_A: 0.791 idt_A: 0.153 D_B: 0.003 G_B: 0.874 cycle_B: 0.808 idt_B: 0.159 \n",
      "saving the model at the end of epoch 180, iters 12240\n",
      "End of epoch 180 / 200 \t Time Taken: 14 sec\n",
      "learning rate 0.0000396 -> 0.0000376\n",
      "(epoch: 181, iters: 60, time: 0.077, data: 0.002) D_A: 0.011 G_A: 0.847 cycle_A: 0.868 idt_A: 0.157 D_B: 0.008 G_B: 0.790 cycle_B: 0.882 idt_B: 0.164 \n",
      "End of epoch 181 / 200 \t Time Taken: 5 sec\n",
      "learning rate 0.0000376 -> 0.0000356\n",
      "End of epoch 182 / 200 \t Time Taken: 5 sec\n",
      "learning rate 0.0000356 -> 0.0000337\n",
      "(epoch: 183, iters: 24, time: 1.180, data: 0.001) D_A: 0.004 G_A: 0.759 cycle_A: 0.836 idt_A: 0.152 D_B: 0.004 G_B: 0.854 cycle_B: 0.804 idt_B: 0.170 \n",
      "End of epoch 183 / 200 \t Time Taken: 6 sec\n",
      "learning rate 0.0000337 -> 0.0000317\n",
      "(epoch: 184, iters: 56, time: 0.093, data: 0.002) D_A: 0.005 G_A: 0.766 cycle_A: 0.758 idt_A: 0.162 D_B: 0.012 G_B: 0.738 cycle_B: 0.997 idt_B: 0.134 \n",
      "End of epoch 184 / 200 \t Time Taken: 5 sec\n",
      "learning rate 0.0000317 -> 0.0000297\n",
      "saving the model at the end of epoch 185, iters 12580\n",
      "End of epoch 185 / 200 \t Time Taken: 13 sec\n",
      "learning rate 0.0000297 -> 0.0000277\n",
      "(epoch: 186, iters: 20, time: 0.082, data: 0.002) D_A: 0.018 G_A: 0.701 cycle_A: 0.711 idt_A: 0.144 D_B: 0.003 G_B: 0.919 cycle_B: 0.752 idt_B: 0.128 \n",
      "End of epoch 186 / 200 \t Time Taken: 5 sec\n",
      "learning rate 0.0000277 -> 0.0000257\n",
      "(epoch: 187, iters: 52, time: 0.068, data: 0.002) D_A: 0.004 G_A: 0.739 cycle_A: 0.776 idt_A: 0.151 D_B: 0.006 G_B: 0.805 cycle_B: 0.807 idt_B: 0.135 \n",
      "End of epoch 187 / 200 \t Time Taken: 5 sec\n",
      "learning rate 0.0000257 -> 0.0000238\n",
      "End of epoch 188 / 200 \t Time Taken: 5 sec\n",
      "learning rate 0.0000238 -> 0.0000218\n",
      "(epoch: 189, iters: 16, time: 1.045, data: 0.002) D_A: 0.004 G_A: 0.855 cycle_A: 0.879 idt_A: 0.170 D_B: 0.008 G_B: 0.790 cycle_B: 0.896 idt_B: 0.168 \n",
      "End of epoch 189 / 200 \t Time Taken: 6 sec\n",
      "learning rate 0.0000218 -> 0.0000198\n",
      "(epoch: 190, iters: 48, time: 0.067, data: 0.001) D_A: 0.007 G_A: 0.808 cycle_A: 0.885 idt_A: 0.177 D_B: 0.005 G_B: 0.913 cycle_B: 0.899 idt_B: 0.152 \n",
      "saving the model at the end of epoch 190, iters 12920\n",
      "End of epoch 190 / 200 \t Time Taken: 25 sec\n",
      "learning rate 0.0000198 -> 0.0000178\n",
      "End of epoch 191 / 200 \t Time Taken: 5 sec\n",
      "learning rate 0.0000178 -> 0.0000158\n",
      "(epoch: 192, iters: 12, time: 0.074, data: 0.002) D_A: 0.011 G_A: 0.797 cycle_A: 0.788 idt_A: 0.149 D_B: 0.009 G_B: 0.766 cycle_B: 0.816 idt_B: 0.133 \n",
      "End of epoch 192 / 200 \t Time Taken: 5 sec\n",
      "learning rate 0.0000158 -> 0.0000139\n",
      "(epoch: 193, iters: 44, time: 0.091, data: 0.001) D_A: 0.003 G_A: 0.888 cycle_A: 0.825 idt_A: 0.142 D_B: 0.004 G_B: 0.594 cycle_B: 0.748 idt_B: 0.143 \n",
      "End of epoch 193 / 200 \t Time Taken: 5 sec\n",
      "learning rate 0.0000139 -> 0.0000119\n",
      "End of epoch 194 / 200 \t Time Taken: 5 sec\n",
      "learning rate 0.0000119 -> 0.0000099\n",
      "(epoch: 195, iters: 8, time: 1.022, data: 0.002) D_A: 0.007 G_A: 0.882 cycle_A: 0.819 idt_A: 0.141 D_B: 0.006 G_B: 0.672 cycle_B: 0.797 idt_B: 0.161 \n",
      "saving the model at the end of epoch 195, iters 13260\n",
      "End of epoch 195 / 200 \t Time Taken: 17 sec\n",
      "learning rate 0.0000099 -> 0.0000079\n",
      "(epoch: 196, iters: 40, time: 0.084, data: 0.000) D_A: 0.004 G_A: 0.822 cycle_A: 0.777 idt_A: 0.148 D_B: 0.018 G_B: 0.766 cycle_B: 0.752 idt_B: 0.140 \n",
      "End of epoch 196 / 200 \t Time Taken: 5 sec\n",
      "learning rate 0.0000079 -> 0.0000059\n",
      "End of epoch 197 / 200 \t Time Taken: 5 sec\n",
      "learning rate 0.0000059 -> 0.0000040\n",
      "(epoch: 198, iters: 4, time: 0.078, data: 0.002) D_A: 0.009 G_A: 0.798 cycle_A: 0.688 idt_A: 0.156 D_B: 0.010 G_B: 0.798 cycle_B: 0.807 idt_B: 0.138 \n",
      "End of epoch 198 / 200 \t Time Taken: 5 sec\n",
      "learning rate 0.0000040 -> 0.0000020\n",
      "(epoch: 199, iters: 36, time: 0.080, data: 0.000) D_A: 0.003 G_A: 0.897 cycle_A: 0.954 idt_A: 0.152 D_B: 0.021 G_B: 0.744 cycle_B: 0.772 idt_B: 0.165 \n",
      "End of epoch 199 / 200 \t Time Taken: 5 sec\n",
      "learning rate 0.0000020 -> 0.0000000\n",
      "(epoch: 200, iters: 68, time: 0.961, data: 0.002) D_A: 0.005 G_A: 0.814 cycle_A: 0.761 idt_A: 0.145 D_B: 0.023 G_B: 0.805 cycle_B: 0.810 idt_B: 0.139 \n",
      "saving the model at the end of epoch 200, iters 13600\n",
      "End of epoch 200 / 200 \t Time Taken: 16 sec\n"
     ]
    }
   ],
   "source": [
    "!python train.py --dataroot /nfs2/xuh11/Connectome/CycleGAN-ipynb/pytorch-CycleGAN-and-pix2pix/datasets/Mean_Length --name BIOCARDtoVMAP_MeanLength --model cycle_gan --display_id -1 --dataset_mode unaligned --input_nc 1 --output_nc 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9UkcaFZiyASl"
   },
   "source": [
    "# Testing\n",
    "\n",
    "-   `python test.py --dataroot datasets/horse2zebra/testA --name horse2zebra_pretrained --model test --no_dropout`\n",
    "\n",
    "Change the `--dataroot` and `--name` to be consistent with your trained model's configuration.\n",
    "\n",
    "> from https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix:\n",
    "> The option --model test is used for generating results of CycleGAN only for one side. This option will automatically set --dataset_mode single, which only loads the images from one set. On the contrary, using --model cycle_gan requires loading and generating results in both directions, which is sometimes unnecessary. The results will be saved at ./results/. Use --results_dir {directory_path_to_save_result} to specify the results directory.\n",
    "\n",
    "> For your own experiments, you might want to specify --netG, --norm, --no_dropout to match the generator architecture of the trained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uCsKkEq0yGh0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------- Options ---------------\n",
      "             aspect_ratio: 1.0                           \n",
      "               batch_size: 1                             \n",
      "          checkpoints_dir: ./checkpoints                 \n",
      "                crop_size: 256                           \n",
      "                 dataroot: /nfs2/xuh11/Connectome/CycleGAN-ipynb/pytorch-CycleGAN-and-pix2pix/datasets/Mean_Length/testA\t[default: None]\n",
      "             dataset_mode: single                        \n",
      "                direction: AtoB                          \n",
      "          display_winsize: 256                           \n",
      "                    epoch: latest                        \n",
      "                     eval: False                         \n",
      "                  gpu_ids: 0                             \n",
      "                init_gain: 0.02                          \n",
      "                init_type: normal                        \n",
      "                 input_nc: 1                             \t[default: 3]\n",
      "                  isTrain: False                         \t[default: None]\n",
      "                load_iter: 0                             \t[default: 0]\n",
      "                load_size: 256                           \n",
      "         max_dataset_size: inf                           \n",
      "                    model: test                          \n",
      "             model_suffix:                               \n",
      "               n_layers_D: 3                             \n",
      "                     name: ./BIOCARDtoVMAP_MeanLength_pretrained\t[default: experiment_name]\n",
      "                      ndf: 64                            \n",
      "                     netD: basic                         \n",
      "                     netG: resnet_9blocks                \n",
      "                      ngf: 64                            \n",
      "               no_dropout: True                          \t[default: False]\n",
      "                  no_flip: False                         \n",
      "                     norm: instance                      \n",
      "                 num_test: 50                            \n",
      "              num_threads: 4                             \n",
      "                output_nc: 1                             \t[default: 3]\n",
      "                    phase: test                          \n",
      "               preprocess: resize_and_crop               \n",
      "              results_dir: ./results/                    \n",
      "           serial_batches: False                         \n",
      "                   suffix:                               \n",
      "                use_wandb: False                         \n",
      "                  verbose: False                         \n",
      "       wandb_project_name: CycleGAN-and-pix2pix          \n",
      "----------------- End -------------------\n",
      "dataset [SingleDataset] was created\n",
      "initialize network with normal\n",
      "model [TestModel] was created\n",
      "loading the model from ./checkpoints/./BIOCARDtoVMAP_MeanLength_pretrained/latest_net_G.pth\n",
      "Traceback (most recent call last):\n",
      "  File \"/nfs2/xuh11/Connectome/CycleGAN-ipynb/pytorch-CycleGAN-and-pix2pix/test.py\", line 52, in <module>\n",
      "    model.setup(opt)               # regular setup: load and print networks; create schedulers\n",
      "    ^^^^^^^^^^^^^^^^\n",
      "  File \"/nfs2/xuh11/Connectome/CycleGAN-ipynb/pytorch-CycleGAN-and-pix2pix/models/base_model.py\", line 88, in setup\n",
      "    self.load_networks(load_suffix)\n",
      "  File \"/nfs2/xuh11/Connectome/CycleGAN-ipynb/pytorch-CycleGAN-and-pix2pix/models/base_model.py\", line 192, in load_networks\n",
      "    state_dict = torch.load(load_path, map_location=str(self.device))\n",
      "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home-local/software/anaconda3/envs/csvToNpy/lib/python3.11/site-packages/torch/serialization.py\", line 791, in load\n",
      "    with _open_file_like(f, 'rb') as opened_file:\n",
      "         ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home-local/software/anaconda3/envs/csvToNpy/lib/python3.11/site-packages/torch/serialization.py\", line 271, in _open_file_like\n",
      "    return _open_file(name_or_buffer, mode)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home-local/software/anaconda3/envs/csvToNpy/lib/python3.11/site-packages/torch/serialization.py\", line 252, in __init__\n",
      "    super().__init__(open(name, mode))\n",
      "                     ^^^^^^^^^^^^^^^^\n",
      "FileNotFoundError: [Errno 2] No such file or directory: './checkpoints/./BIOCARDtoVMAP_MeanLength_pretrained/latest_net_G.pth'\n"
     ]
    }
   ],
   "source": [
    "!python test.py \\\n",
    "--dataroot /nfs2/xuh11/Connectome/CycleGAN-ipynb/pytorch-CycleGAN-and-pix2pix/datasets/Mean_Length/testA \\\n",
    "--name BIOCARDtoVMAP_MeanLength_pretrained \\\n",
    "--model test \\\n",
    "--no_dropout \\\n",
    "--input_nc 1 \\\n",
    "--output_nc 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OzSKIPUByfiN"
   },
   "source": [
    "# Visualize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9Mgg8raPyizq"
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './results/horse2zebra_pretrained/test_latest/images/n02381460_1010_fake.png'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m img \u001b[38;5;241m=\u001b[39m plt\u001b[38;5;241m.\u001b[39mimread(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./results/horse2zebra_pretrained/test_latest/images/n02381460_1010_fake.png\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      4\u001b[0m plt\u001b[38;5;241m.\u001b[39mimshow(img)\n",
      "File \u001b[0;32m/home-local/software/anaconda3/envs/csvToNpy/lib/python3.11/site-packages/matplotlib/pyplot.py:2195\u001b[0m, in \u001b[0;36mimread\u001b[0;34m(fname, format)\u001b[0m\n\u001b[1;32m   2193\u001b[0m \u001b[39m@_copy_docstring_and_deprecators\u001b[39m(matplotlib\u001b[39m.\u001b[39mimage\u001b[39m.\u001b[39mimread)\n\u001b[1;32m   2194\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mimread\u001b[39m(fname, \u001b[39mformat\u001b[39m\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[0;32m-> 2195\u001b[0m     \u001b[39mreturn\u001b[39;00m matplotlib\u001b[39m.\u001b[39mimage\u001b[39m.\u001b[39mimread(fname, \u001b[39mformat\u001b[39m)\n",
      "File \u001b[0;32m/home-local/software/anaconda3/envs/csvToNpy/lib/python3.11/site-packages/matplotlib/image.py:1563\u001b[0m, in \u001b[0;36mimread\u001b[0;34m(fname, format)\u001b[0m\n\u001b[1;32m   1556\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(fname, \u001b[39mstr\u001b[39m) \u001b[39mand\u001b[39;00m \u001b[39mlen\u001b[39m(parse\u001b[39m.\u001b[39murlparse(fname)\u001b[39m.\u001b[39mscheme) \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m   1557\u001b[0m     \u001b[39m# Pillow doesn't handle URLs directly.\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m   1559\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mPlease open the URL for reading and pass the \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1560\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mresult to Pillow, e.g. with \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1561\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m``np.array(PIL.Image.open(urllib.request.urlopen(url)))``.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1562\u001b[0m         )\n\u001b[0;32m-> 1563\u001b[0m \u001b[39mwith\u001b[39;00m img_open(fname) \u001b[39mas\u001b[39;00m image:\n\u001b[1;32m   1564\u001b[0m     \u001b[39mreturn\u001b[39;00m (_pil_png_to_float_array(image)\n\u001b[1;32m   1565\u001b[0m             \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(image, PIL\u001b[39m.\u001b[39mPngImagePlugin\u001b[39m.\u001b[39mPngImageFile) \u001b[39melse\u001b[39;00m\n\u001b[1;32m   1566\u001b[0m             pil_to_array(image))\n",
      "File \u001b[0;32m/home-local/software/anaconda3/envs/csvToNpy/lib/python3.11/site-packages/PIL/ImageFile.py:105\u001b[0m, in \u001b[0;36mImageFile.__init__\u001b[0;34m(self, fp, filename)\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdecodermaxblock \u001b[39m=\u001b[39m MAXBLOCK\n\u001b[1;32m    103\u001b[0m \u001b[39mif\u001b[39;00m is_path(fp):\n\u001b[1;32m    104\u001b[0m     \u001b[39m# filename\u001b[39;00m\n\u001b[0;32m--> 105\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfp \u001b[39m=\u001b[39m \u001b[39mopen\u001b[39m(fp, \u001b[39m\"\u001b[39m\u001b[39mrb\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    106\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfilename \u001b[39m=\u001b[39m fp\n\u001b[1;32m    107\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_exclusive_fp \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './results/horse2zebra_pretrained/test_latest/images/n02381460_1010_fake.png'"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "img = plt.imread('./results/horse2zebra_pretrained/test_latest/images/n02381460_1010_fake.png')\n",
    "plt.imshow(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0G3oVH9DyqLQ"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "img = plt.imread('./results/horse2zebra_pretrained/test_latest/images/n02381460_1010_real.png')\n",
    "plt.imshow(img)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "CycleGAN",
   "provenance": []
  },
  "environment": {
   "name": "tf2-gpu.2-3.m74",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-3:m74"
  },
  "kernelspec": {
   "display_name": "Python 3.11.3 ('csvToNpy')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "vscode": {
   "interpreter": {
    "hash": "5803b030e1520f9a7dad528272dd9ce524b86bf4b74738c6a562460697dfb6f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
